{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1c386c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e69135",
   "metadata": {},
   "source": [
    "LOAD ALL MODEL RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243cb3fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Could not find results file - [Errno 2] No such file or directory: 'mlp_results.csv'\n",
      "Please ensure all models have been trained and results saved.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "try:\n",
    "    mlp_results = pd.read_csv('mlp_results.csv')\n",
    "    autoencoder_results = pd.read_csv('autoencoder_results.csv')\n",
    "    lstm_results = pd.read_csv('lstm_results.csv')\n",
    "    cnn1d_results = pd.read_csv('cnn1d_results.csv')\n",
    "    \n",
    "    # Combine all results\n",
    "    all_results = pd.concat([mlp_results, autoencoder_results, \n",
    "                             lstm_results, cnn1d_results], ignore_index=True)\n",
    "    \n",
    "    print(\"Successfully loaded all model results!\")\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"COMPREHENSIVE RESULTS TABLE\")\n",
    "    print(\"=\"*70)\n",
    "    print(all_results.to_string(index=False))\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: Could not find results file - {e}\")\n",
    "    print(\"Please ensure all models have been trained and results saved.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2977f8e6",
   "metadata": {},
   "source": [
    "PERFORMANCE COMPARISON VISUALIZATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45d8cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (15, 10)\n",
    "\n",
    "# Metrics to compare\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']\n",
    "models = all_results['Model'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722b5ad2",
   "metadata": {},
   "source": [
    "BAR CHART COMPARISON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd8b54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, metric in enumerate(metrics):\n",
    "    values = all_results[metric].values\n",
    "    colors = ['#1f77b4', '#2ca02c', '#9467bd', '#ff7f0e']\n",
    "    \n",
    "    axes[idx].bar(models, values, color=colors, alpha=0.8, edgecolor='black')\n",
    "    axes[idx].set_title(f'{metric} Comparison', fontsize=14, fontweight='bold')\n",
    "    axes[idx].set_ylabel(metric, fontsize=12)\n",
    "    axes[idx].set_ylim([0, 1.05])\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, v in enumerate(values):\n",
    "        axes[idx].text(i, v + 0.02, f'{v:.4f}', ha='center', fontsize=10)\n",
    "    \n",
    "    # Rotate x-axis labels\n",
    "    axes[idx].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Remove extra subplot\n",
    "axes[-1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_comparison_bars.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79856ffe",
   "metadata": {},
   "source": [
    "RADAR CHART COMPARISON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753039d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import pi\n",
    "\n",
    "# Number of metrics\n",
    "categories = metrics\n",
    "N = len(categories)\n",
    "\n",
    "# Compute angle for each axis\n",
    "angles = [n / float(N) * 2 * pi for n in range(N)]\n",
    "angles += angles[:1]\n",
    "\n",
    "# Initialize plot\n",
    "fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))\n",
    "\n",
    "# Plot each model\n",
    "colors = ['#1f77b4', '#2ca02c', '#9467bd', '#ff7f0e']\n",
    "for idx, model in enumerate(models):\n",
    "    values = all_results.iloc[idx][metrics].values.tolist()\n",
    "    values += values[:1]\n",
    "    \n",
    "    ax.plot(angles, values, 'o-', linewidth=2, label=model, color=colors[idx])\n",
    "    ax.fill(angles, values, alpha=0.15, color=colors[idx])\n",
    "\n",
    "# Fix axis to go in the right order and start at 12 o'clock\n",
    "ax.set_theta_offset(pi / 2)\n",
    "ax.set_theta_direction(-1)\n",
    "\n",
    "# Draw axis lines for each angle and label\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(categories, fontsize=11)\n",
    "\n",
    "# Set y-axis limits\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "ax.set_yticklabels(['0.2', '0.4', '0.6', '0.8', '1.0'], fontsize=9)\n",
    "\n",
    "# Add legend\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1), fontsize=11)\n",
    "plt.title('Model Performance Radar Chart', size=16, fontweight='bold', pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_comparison_radar.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e0e914",
   "metadata": {},
   "source": [
    "HEATMAP COMPARISON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d98daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for heatmap\n",
    "heatmap_data = all_results[metrics].T\n",
    "heatmap_data.columns = models\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(heatmap_data, annot=True, fmt='.4f', cmap='YlGnBu', \n",
    "            cbar_kws={'label': 'Score'}, linewidths=0.5, linecolor='gray',\n",
    "            vmin=0, vmax=1)\n",
    "plt.title('Model Performance Heatmap', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.xlabel('Models', fontsize=12)\n",
    "plt.ylabel('Metrics', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_comparison_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d043aa",
   "metadata": {},
   "source": [
    "GROUPED BAR CHART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7993db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(len(models))\n",
    "width = 0.15\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    values = all_results[metric].values\n",
    "    ax.bar(x + i * width, values, width, label=metric, alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Models', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Scores', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Grouped Performance Comparison', fontsize=16, fontweight='bold')\n",
    "ax.set_xticks(x + width * 2)\n",
    "ax.set_xticklabels(models)\n",
    "ax.legend(loc='lower right', fontsize=10)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "ax.set_ylim([0, 1.05])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_comparison_grouped.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b62454b",
   "metadata": {},
   "source": [
    "STATISTICAL SUMMARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b92be98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best model for each metric\n",
    "print(\"\\nBest Model for Each Metric:\")\n",
    "print(\"-\" * 70)\n",
    "for metric in metrics:\n",
    "    best_idx = all_results[metric].idxmax()\n",
    "    best_model = all_results.loc[best_idx, 'Model']\n",
    "    best_score = all_results.loc[best_idx, metric]\n",
    "    print(f\"{metric:15s}: {best_model:20s} ({best_score:.4f})\")\n",
    "\n",
    "# Overall best model (based on average performance)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"OVERALL BEST MODEL (Average Performance)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "all_results['Average'] = all_results[metrics].mean(axis=1)\n",
    "best_overall_idx = all_results['Average'].idxmax()\n",
    "best_overall_model = all_results.loc[best_overall_idx, 'Model']\n",
    "best_overall_score = all_results.loc[best_overall_idx, 'Average']\n",
    "\n",
    "print(f\"\\nBest Model: {best_overall_model}\")\n",
    "print(f\"Average Score: {best_overall_score:.4f}\")\n",
    "print(\"\\nDetailed Scores:\")\n",
    "for metric in metrics:\n",
    "    score = all_results.loc[best_overall_idx, metric]\n",
    "    print(f\"  {metric:12s}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b62e87",
   "metadata": {},
   "source": [
    "RANKING TABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa6fc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking_df = pd.DataFrame()\n",
    "ranking_df['Model'] = models\n",
    "\n",
    "for metric in metrics:\n",
    "    ranks = all_results[metric].rank(ascending=False).astype(int)\n",
    "    ranking_df[metric] = ranks\n",
    "\n",
    "ranking_df['Average Rank'] = ranking_df[metrics].mean(axis=1)\n",
    "ranking_df = ranking_df.sort_values('Average Rank')\n",
    "\n",
    "print(\"\\n\" + ranking_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e59bd63",
   "metadata": {},
   "source": [
    "PERCENTAGE DIFFERENCES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57a2be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "difference_df = pd.DataFrame()\n",
    "difference_df['Model'] = models\n",
    "\n",
    "for metric in metrics:\n",
    "    max_val = all_results[metric].max()\n",
    "    differences = ((all_results[metric] - max_val) / max_val * 100).round(2)\n",
    "    difference_df[metric] = differences.astype(str) + '%'\n",
    "\n",
    "print(\"\\n\" + difference_df.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
